{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def clear_auto(element):\n",
    "    auto_input = str(element.get_attribute(\"value\"))\n",
    "    auto_length = len(auto_input)\n",
    "    # Delete everything\n",
    "    for i in range(auto_length):\n",
    "        element.send_keys(Keys.BACKSPACE)\n",
    "        \n",
    "def indeed_query(title, location, dirty_jobs):\n",
    "    # Go to Indeed homepage\n",
    "    global browser\n",
    "    browser.get(\"https://www.indeed.com/\")\n",
    "    print('Scraping:', title, '-', location)\n",
    "    # Find input fields\n",
    "    what = browser.find_element_by_name(\"q\")\n",
    "    what.send_keys(str(title))\n",
    "    where = browser.find_element_by_name(\"l\")\n",
    "    clear_auto(where)\n",
    "    where.send_keys(str(location))\n",
    "\n",
    "    # Click Search\n",
    "    button = browser.find_element_by_class_name(\"icl-Button\")\n",
    "    button.click()\n",
    "\n",
    "    # Collect pages to scrape\n",
    "    url = browser.current_url\n",
    "\n",
    "    # Get soup for the results page\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # extract job results\n",
    "    for link in soup.find_all('a', {'class':'jobtitle turnstileLink'}):\n",
    "        dirty_jobs.append(link.attrs['href'])\n",
    "        \n",
    "def clean_jobs(jobs):\n",
    "    # Clean clean clean\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "            \n",
    "    # add prefix\n",
    "    jobs = ['https://www.indeed.com' + job for job in jobs]\n",
    "            \n",
    "    return jobs\n",
    "\n",
    "def indeed_crawl(titles, locations, dirty_jobs):\n",
    "\n",
    "    # Loop locations\n",
    "    for location in locations:\n",
    "        # Loop titles\n",
    "        for title in titles:\n",
    "            indeed_query(title, location, dirty_jobs)\n",
    "\n",
    "    # Clean it up\n",
    "    jobs = clean_jobs(dirty_jobs)\n",
    "    return jobs\n",
    "\n",
    "def clean_description(text):\n",
    "    # Clean\n",
    "    clean_text = BeautifulSoup(text, \"lxml\").text\n",
    "    # clean_text = clean_text[2:]\n",
    "    clean_text = re.sub(r'\\\\n', ' ', clean_text)\n",
    "    clean_text = re.sub(r'/', ' ', clean_text)\n",
    "    clean_text = re.sub(r'[^a-zA-Z ^0-9]', '', clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenize\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]\n",
    "    if ' ' in tokens:\n",
    "        tokens.remove(' ')\n",
    "    if '  ' in tokens:\n",
    "        tokens.remove('  ')\n",
    "    return tokens\n",
    "\n",
    "def fit_for_nn(text_list):\n",
    "    # Create a vocab and get word counts per doc\n",
    "    sparse = tfidf.fit_transform(text)\n",
    "    # send to df\n",
    "    tfidf_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())\n",
    "    return tfidf_dtm\n",
    "    \n",
    "def process_query_for_nn(string):\n",
    "    # Create a vocab and get word counts per doc\n",
    "    sparse = tfidf.transform([query])\n",
    "    query_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())\n",
    "    return query_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin-Up Web Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: use options instead of chrome_options\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Specifying incognito mode as you launch your browser[OPTIONAL]\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\"--incognito\")\n",
    "    \n",
    "# Create new Instance of Chrome in incognito mode\n",
    "browser = webdriver.Chrome(executable_path='../Documents/Web Drivers/chromedriver', chrome_options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles and locations\n",
    "titles = [\"Data Analyst\", \"Data Scientist\"]\n",
    "locations = [\"Raleigh-Durham, NC\", \"Charlotte, NC\", \"Roanoke, VA\", \"Charlottesville, VA\",\n",
    "             \"Greensboro, NC\", \"Winston-Salem, NC\", \"Annapolis, MD\", \"Philadelphia, PA\",\n",
    "             \"Pittsburgh, PA\", \"Manassas, VA\", \"Herndon, VA\", \"Washington, DC\", \"Arlington, VA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Data Analyst - Raleigh-Durham, NC\n",
      "Scraping: Data Scientist - Raleigh-Durham, NC\n",
      "Scraping: Data Analyst - Charlotte, NC\n",
      "Scraping: Data Scientist - Charlotte, NC\n",
      "Scraping: Data Analyst - Roanoke, VA\n",
      "Scraping: Data Scientist - Roanoke, VA\n",
      "Scraping: Data Analyst - Charlottesville, VA\n",
      "Scraping: Data Scientist - Charlottesville, VA\n",
      "Scraping: Data Analyst - Greensboro, NC\n",
      "Scraping: Data Scientist - Greensboro, NC\n",
      "Scraping: Data Analyst - Winston-Salem, NC\n",
      "Scraping: Data Scientist - Winston-Salem, NC\n",
      "Scraping: Data Analyst - Annapolis, MD\n",
      "Scraping: Data Scientist - Annapolis, MD\n",
      "Scraping: Data Analyst - Philadelphia, PA\n",
      "Scraping: Data Scientist - Philadelphia, PA\n",
      "Scraping: Data Analyst - Pittsburgh, PA\n",
      "Scraping: Data Scientist - Pittsburgh, PA\n",
      "Scraping: Data Analyst - Manassas, VA\n",
      "Scraping: Data Scientist - Manassas, VA\n",
      "Scraping: Data Analyst - Herndon, VA\n",
      "Scraping: Data Scientist - Herndon, VA\n",
      "Scraping: Data Analyst - Washington, DC\n",
      "Scraping: Data Scientist - Washington, DC\n",
      "Scraping: Data Analyst - Arlington, VA\n",
      "Scraping: Data Scientist - Arlington, VA\n"
     ]
    }
   ],
   "source": [
    "# Instantiate dirty_jobs\n",
    "dirty_jobs = []\n",
    "\n",
    "# Crawl over indeed.com\n",
    "jobs = indeed_crawl(titles, locations, dirty_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button class=\"icl-Button icl-Button--primary icl-Button--md icl-WhatWhere-button\" size=\"md\" type=\"submit\">Find Jobs</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    html = urlopen(job)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    texts.append(soup.find_all('div', {'class':'jobsearch-jobDescriptionText'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [str(text)[1:-1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>&lt;div class=\"jobsearch-jobDescriptionText\" id=\"...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=3a77ced4e1f6c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>&lt;div class=\"jobsearch-jobDescriptionText\" id=\"...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=b20e16df83866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>&lt;div class=\"jobsearch-jobDescriptionText\" id=\"...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=bf57b8d389d71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>&lt;div class=\"jobsearch-jobDescriptionText\" id=\"...</td>\n",
       "      <td>https://www.indeed.com/company/Lakarya/jobs/Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>&lt;div class=\"jobsearch-jobDescriptionText\" id=\"...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=49e950e1919c2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  <div class=\"jobsearch-jobDescriptionText\" id=\"...   \n",
       "1  <div class=\"jobsearch-jobDescriptionText\" id=\"...   \n",
       "2  <div class=\"jobsearch-jobDescriptionText\" id=\"...   \n",
       "3  <div class=\"jobsearch-jobDescriptionText\" id=\"...   \n",
       "4  <div class=\"jobsearch-jobDescriptionText\" id=\"...   \n",
       "\n",
       "                                                jobs  \n",
       "0  https://www.indeed.com/rc/clk?jk=3a77ced4e1f6c...  \n",
       "1  https://www.indeed.com/rc/clk?jk=b20e16df83866...  \n",
       "2  https://www.indeed.com/rc/clk?jk=bf57b8d389d71...  \n",
       "3  https://www.indeed.com/company/Lakarya/jobs/Da...  \n",
       "4  https://www.indeed.com/rc/clk?jk=49e950e1919c2...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send to df\n",
    "df = pd.DataFrame(texts, columns = ['description'])\n",
    "df['jobs'] = jobs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and tokenize the descriptions\n",
    "df['tokens'] = df['description'].apply(clean_description).apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send clean text to list\n",
    "text = df['description'].apply(clean_description).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dtm = fit_for_nn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab and get word counts per doc\n",
    "sparse = tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to df\n",
    "tfidf_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "nn = NearestNeighbors(n_neighbors=20, algorithm='ball_tree')\n",
    "nn.fit(tfidf_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"I use python to collect and scrape data from the web. I can set up integrated data pipelines\n",
    "        pipeline to collect data from different sources. I train machine learning models using sklearn, \n",
    "        and tensorflow with keras. BeautifulSoup and Seleium. I can give results to developers using Flask apps\n",
    "        and Flask APIs API. I can access APIs API and RSS feeds. I can also use SQL, particularly ElephantSQL\n",
    "        and PostgreSQL. I like venture capital, finance and business consulting. Looking for a junior\n",
    "        or entry level entry-level or mid level mid-level\"\"\"\n",
    "\n",
    "query_dtm = process_query_for_nn(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for closest neighbors\n",
    "results = nn.kneighbors(query_dtm)[1][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.indeed.com/rc/clk?jk=b9ccc987c4c7ee97&fccid=8fd36856966c7aad&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=1e45d5b3889a4f68&fccid=4e041af1d0af1bc8&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=fb893c7289cb8935&fccid=a3163e1f583839d0&vjs=3',\n",
       " 'https://www.indeed.com/company/Quantum-Technologies-Inc./jobs/Data-Analyst-c6475c487dca36d7?fccid=e169cdbdee4fdc2e&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=2986305e03bcd41c&fccid=dc2638a079b95f93&vjs=3',\n",
       " 'https://www.indeed.com/company/Visual-Bridge/jobs/Technology-Consultant-14a052d192068fb2?fccid=22ade5f3d42c0d41&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=5f98296a165e5454&fccid=e8f18ca6180ec8da&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=5f98296a165e5454&fccid=e8f18ca6180ec8da&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=1a7ce6158404da8a&fccid=dfc44f3b8c44a6db&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=3d46376452926cef&fccid=a168335bbdcce5e0&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=ce6c891fea3c8615&fccid=4e041af1d0af1bc8&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=ce6c891fea3c8615&fccid=4e041af1d0af1bc8&vjs=3',\n",
       " 'https://www.indeed.com/company/Meridian-Technologies/jobs/Data-Scientist-fcb6044b2c90cca5?fccid=30e2c939cb512212&vjs=3',\n",
       " 'https://www.indeed.com/company/Systems-Ltd/jobs/Data-Analyst-b7575463ba71db1e?fccid=e4ff19df51470355&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=da339af51cc7ef6c&fccid=89db72b7484b93bb&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=322eb513890aab74&fccid=4e041af1d0af1bc8&vjs=3',\n",
       " 'https://www.indeed.com/company/Amick-Brown,-LLC/jobs/Data-Engineer-2ec851a00cc54ce3?fccid=eedf72ea6f271445&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=125d9d6f819505c7&fccid=4e041af1d0af1bc8&vjs=3',\n",
       " 'https://www.indeed.com/company/Dhatronictech/jobs/Entry-Level-Business-Analyst-3ade49b0c89bb5d4?fccid=2ab83a6f82c0e82b&vjs=3',\n",
       " 'https://www.indeed.com/company/Dhatronictech/jobs/Entry-Level-Business-Analyst-3ade49b0c89bb5d4?fccid=2ab83a6f82c0e82b&vjs=3']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['jobs'][results].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
