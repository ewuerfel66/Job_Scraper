{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def clear_auto(element):\n",
    "    auto_input = str(element.get_attribute(\"value\"))\n",
    "    auto_length = len(auto_input)\n",
    "    # Delete everything\n",
    "    for i in range(auto_length):\n",
    "        element.send_keys(Keys.BACKSPACE)\n",
    "        \n",
    "def indeed_query(title, location, dirty_jobs):\n",
    "    # Go to Indeed homepage\n",
    "    global browser\n",
    "    browser.get(\"https://www.indeed.com/\")\n",
    "    print('Scraping:', title, '-', location)\n",
    "    # Find input fields\n",
    "    what = browser.find_element_by_name(\"q\")\n",
    "    what.send_keys(str(title))\n",
    "    where = browser.find_element_by_name(\"l\")\n",
    "    clear_auto(where)\n",
    "    where.send_keys(str(location))\n",
    "\n",
    "    # Click Search\n",
    "    button = browser.find_element_by_class_name(\"icl-Button\")\n",
    "    button.click()\n",
    "\n",
    "    # Collect pages to scrape\n",
    "    url = browser.current_url\n",
    "\n",
    "    # Get soup for the results page\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # extract job results\n",
    "    for link in soup.find_all('a', {'class':'jobtitle turnstileLink'}):\n",
    "        dirty_jobs.append(link.attrs['href'])\n",
    "        \n",
    "def deep_indeed_query(title, location, dirty_jobs):\n",
    "    # Go to Indeed homepage\n",
    "    global browser\n",
    "    browser.get(\"https://www.indeed.com/\")\n",
    "    print('Scraping:', title, '-', location)\n",
    "    # Find input fields\n",
    "    what = browser.find_element_by_name(\"q\")\n",
    "    what.send_keys(str(title))\n",
    "    where = browser.find_element_by_name(\"l\")\n",
    "    clear_auto(where)\n",
    "    where.send_keys(str(location))\n",
    "\n",
    "    # Click Search\n",
    "    button = browser.find_element_by_class_name(\"icl-Button\")\n",
    "    button.click()\n",
    "\n",
    "    for i in range(3):\n",
    "        # Collect pages to scrape\n",
    "        url = browser.current_url\n",
    "\n",
    "        # Get soup for the results page\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # extract job results\n",
    "        for link in soup.find_all('a', {'class':'jobtitle turnstileLink'}):\n",
    "            dirty_jobs.append(link.attrs['href'])\n",
    "            \n",
    "        # click on next page\n",
    "        try:\n",
    "            next_page = browser.find_element_by_class_name(\"np\")\n",
    "            next_page.click()\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "        \n",
    "def clean_jobs(jobs):\n",
    "    # Find unique values\n",
    "    jobs = list(set(jobs))\n",
    "    \n",
    "    # Clean clean clean\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job.startswith('/pagead'):\n",
    "            jobs.remove(job)\n",
    "            \n",
    "    # add prefix\n",
    "    jobs = ['https://www.indeed.com' + job for job in jobs]\n",
    "            \n",
    "    return jobs\n",
    "\n",
    "def indeed_crawl(titles, locations, dirty_jobs):\n",
    "\n",
    "    # Loop locations\n",
    "    for location in locations:\n",
    "        # Loop titles\n",
    "        for title in titles:\n",
    "            deep_indeed_query(title, location, dirty_jobs)\n",
    "\n",
    "    # Clean it up\n",
    "    jobs = clean_jobs(dirty_jobs)\n",
    "    return jobs\n",
    "\n",
    "def clean_description(text):\n",
    "    # Clean\n",
    "    clean_text = BeautifulSoup(text, \"lxml\").text\n",
    "    # clean_text = clean_text[2:]\n",
    "    clean_text = re.sub(r'\\\\n', ' ', clean_text)\n",
    "    clean_text = re.sub(r'/', ' ', clean_text)\n",
    "    clean_text = re.sub(r'[^a-zA-Z ^0-9]', '', clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenize\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]\n",
    "    if ' ' in tokens:\n",
    "        tokens.remove(' ')\n",
    "    if '  ' in tokens:\n",
    "        tokens.remove('  ')\n",
    "    return tokens\n",
    "\n",
    "def fit_for_nn(text_list):\n",
    "    # Create a vocab and get word counts per doc\n",
    "    sparse = tfidf.fit_transform(text)\n",
    "    # send to df\n",
    "    tfidf_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())\n",
    "    return tfidf_dtm\n",
    "    \n",
    "def process_query_for_nn(string):\n",
    "    # Create a vocab and get word counts per doc\n",
    "    sparse = tfidf.transform([query])\n",
    "    query_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())\n",
    "    return query_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_indeed_query(title, location, dirty_jobs):\n",
    "    # Go to Indeed homepage\n",
    "    global browser\n",
    "    browser.get(\"https://www.indeed.com/\")\n",
    "    print('Scraping:', title, '-', location)\n",
    "    # Find input fields\n",
    "    what = browser.find_element_by_name(\"q\")\n",
    "    what.send_keys(str(title))\n",
    "    where = browser.find_element_by_name(\"l\")\n",
    "    clear_auto(where)\n",
    "    where.send_keys(str(location))\n",
    "\n",
    "    # Click Search\n",
    "    button = browser.find_element_by_class_name(\"icl-Button\")\n",
    "    button.click()\n",
    "\n",
    "    for i in range(3):\n",
    "        # Collect pages to scrape\n",
    "        url = browser.current_url\n",
    "\n",
    "        # Get soup for the results page\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # extract job results\n",
    "        for link in soup.find_all('a', {'class':'jobtitle turnstileLink'}):\n",
    "            dirty_jobs.append(link.attrs['href'])\n",
    "            \n",
    "        # exit popup if it comes up\n",
    "        try:\n",
    "            popup_x = browser.find_element_by_class_name(\"icl-Icon icl-Icon--sm  icl-Icon--black close\")\n",
    "            popup_x.click()\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "        \n",
    "        # click on next page\n",
    "        try:\n",
    "            next_page = browser.find_element_by_class_name(\"np\")\n",
    "            next_page.click()\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "            \n",
    "# <svg role=\"img\" class=\"icl-Icon icl-Icon--sm  icl-Icon--black close\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin-Up Web Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Disable auto-complete\n",
    "profile = webdriver.FirefoxProfile()\n",
    "profile.set_preference(\"browser.formfill.enable\", \"false\")\n",
    "    \n",
    "# Create new Instance of Chrome in incognito mode\n",
    "browser = webdriver.Firefox(executable_path='../../Selenium/geckodriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles and locations\n",
    "titles = [\"Data Engineer\", \"Data Analyst\", \"Data Scientist\"]\n",
    "locations = [\"Raleigh-Durham, NC\", \"Charlotte, NC\", \"Roanoke, VA\", \"Charlottesville, VA\",\n",
    "             \"Greensboro, NC\", \"Winston-Salem, NC\", \"Annapolis, MD\", \"Philadelphia, PA\",\n",
    "             \"Tyson's Corner, VA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Data Engineer - Raleigh-Durham, NC\n",
      "Scraping: Data Analyst - Raleigh-Durham, NC\n",
      "Scraping: Data Scientist - Raleigh-Durham, NC\n",
      "Scraping: Data Engineer - Charlotte, NC\n",
      "Scraping: Data Analyst - Charlotte, NC\n",
      "Scraping: Data Scientist - Charlotte, NC\n",
      "Scraping: Data Engineer - Roanoke, VA\n",
      "Scraping: Data Analyst - Roanoke, VA\n",
      "Scraping: Data Scientist - Roanoke, VA\n",
      "Scraping: Data Engineer - Charlottesville, VA\n",
      "Scraping: Data Analyst - Charlottesville, VA\n",
      "Scraping: Data Scientist - Charlottesville, VA\n",
      "Scraping: Data Engineer - Greensboro, NC\n",
      "Scraping: Data Analyst - Greensboro, NC\n",
      "Scraping: Data Scientist - Greensboro, NC\n",
      "Scraping: Data Engineer - Winston-Salem, NC\n",
      "Scraping: Data Analyst - Winston-Salem, NC\n",
      "Scraping: Data Scientist - Winston-Salem, NC\n",
      "Scraping: Data Engineer - Annapolis, MD\n",
      "Scraping: Data Analyst - Annapolis, MD\n",
      "Scraping: Data Scientist - Annapolis, MD\n",
      "Scraping: Data Engineer - Philadelphia, PA\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: The element reference of <input id=\"text-input-what\" class=\"icl-TextInput-control icl-TextInput-control--whatWhere\" name=\"q\" type=\"text\"> is stale; either the element is no longer attached to the DOM, it is not in the current frame context, or the document has been refreshed\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d548ae94bb96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Crawl over indeed.com\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindeed_crawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirty_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4202fb64f934>\u001b[0m in \u001b[0;36mindeed_crawl\u001b[0;34m(titles, locations, dirty_jobs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Loop titles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdeep_indeed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirty_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Clean it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f89c982e5ae6>\u001b[0m in \u001b[0;36mdeep_indeed_query\u001b[0;34m(title, location, dirty_jobs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Find input fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mwhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"q\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mwhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mwhere\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"l\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclear_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36msend_keys\u001b[0;34m(self, *value)\u001b[0m\n\u001b[1;32m    477\u001b[0m         self._execute(Command.SEND_KEYS_TO_ELEMENT,\n\u001b[1;32m    478\u001b[0m                       {'text': \"\".join(keys_to_typing(value)),\n\u001b[0;32m--> 479\u001b[0;31m                        'value': keys_to_typing(value)})\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# RenderedWebElement Items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m: Message: The element reference of <input id=\"text-input-what\" class=\"icl-TextInput-control icl-TextInput-control--whatWhere\" name=\"q\" type=\"text\"> is stale; either the element is no longer attached to the DOM, it is not in the current frame context, or the document has been refreshed\n"
     ]
    }
   ],
   "source": [
    "# Instantiate dirty_jobs\n",
    "dirty_jobs = []\n",
    "\n",
    "# Crawl over indeed.com\n",
    "jobs = indeed_crawl(titles, locations, dirty_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button class=\"icl-Button icl-Button--primary icl-Button--md icl-WhatWhere-button\" size=\"md\" type=\"submit\">Find Jobs</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jobs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a5d36564e6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'jobsearch-jobDescriptionText'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jobs' is not defined"
     ]
    }
   ],
   "source": [
    "for job in jobs:\n",
    "    html = urlopen(job)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    texts.append(soup.find_all('div', {'class':'jobsearch-jobDescriptionText'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [str(text)[1:-1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to df\n",
    "df = pd.DataFrame(texts, columns = ['description'])\n",
    "df['jobs'] = jobs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and tokenize the descriptions\n",
    "df['tokens'] = df['description'].apply(clean_description).apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send clean text to list\n",
    "text = df['description'].apply(clean_description).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dtm = fit_for_nn(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocab and get word counts per doc\n",
    "sparse = tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to df\n",
    "tfidf_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "nn = NearestNeighbors(n_neighbors=20, algorithm='ball_tree')\n",
    "nn.fit(tfidf_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"I use python to collect and scrape data from the web. I can set up integrated data pipelines\n",
    "        pipeline to collect data from different sources. I train machine learning models using sklearn, \n",
    "        and tensorflow with keras. BeautifulSoup and Selenium. BeautifulSoup and Selenium.\n",
    "        BeautifulSoup and Selenium. BeautifulSoup and Selenium. I can give results to developers using Flask apps\n",
    "        and Flask APIs API. I can access APIs API and RSS feeds. I can also use SQL, particularly ElephantSQL\n",
    "        and Postgres. I like venture capital, finance and business consulting. I love to work with\n",
    "        natural language processing. Looking for a junior or entry level entry-level or mid level mid-level\n",
    "        venture capital, finance and business consulting venture capital, finance and business consulting\n",
    "        venture capital, business data solutions, finance and business consulting, \n",
    "        create visualizations with tableau\"\"\"\n",
    "\n",
    "# query = \"\"\"I use knowledge of process and chemical engineering to help businesses optimize production,\n",
    "#         often making use of Statistical Process Control.  Background in math, science, organic chemistry.\n",
    "#         Interested in the environmental waste section, as chemistry forms the backbone of much of that work.\"\"\"\n",
    "\n",
    "query_dtm = process_query_for_nn(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for closest neighbors\n",
    "results = nn.kneighbors(query_dtm)[1][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Send to list\n",
    "job_urls = df['jobs'][results].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val, val)\n",
    "\n",
    "jobs_df = pd.DataFrame(job_urls)\n",
    "\n",
    "jobs_df.style.format(make_clickable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['<a href=\"{' + link +'}\">{' + link + '}</a>' for link in job_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
