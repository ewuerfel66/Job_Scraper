{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "--- Job Scraper ---\n",
      "-------------------\n",
      "\n",
      "A browser will open in a second, let it do it's thing.\n",
      "Crawling Indeed:\n",
      "Scraping: Data Analyst - Raleigh-Durham, NC\n",
      "Scraping: Data Analyst - Charlotte, NC\n",
      "Scraping: Data Analyst - Roanoke, VA\n",
      "Scraping: Data Analyst - Charlottesville, VA\n",
      "Scraping: Data Analyst - Greensboro, NC\n",
      "Parsing descriptions...\n",
      "You can close that browser now\n",
      "Loading Natural Language model...\n",
      "Done loading model!\n",
      "Tokenizing the data...\n",
      "Fitting vectorizer...\n",
      "Teaching the computer...\n",
      "Damn, that computer is smart.\n",
      "Asking the computer for recommendations...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Heroku app that scrapes job ads for permutations of locations & titles.\n",
    "\n",
    "User Input: Location List, Title List, Job Description\n",
    "\n",
    "Output: 20 Job Postings\n",
    "\"\"\"\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------------\")\n",
    "print(\"--- Job Scraper ---\")\n",
    "print(\"-------------------\")\n",
    "print(\"\")\n",
    "print(\"A browser will open in a second, let it do it's thing.\")\n",
    "\n",
    "#######################\n",
    "####### Imports #######\n",
    "#######################\n",
    "\n",
    "# The usuals\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# System Imports\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Functions (Local Imports)\n",
    "from functions import *\n",
    "\n",
    "\n",
    "###############################################\n",
    "############ Get input values #################\n",
    "###############################################\n",
    "\n",
    "# Titles and locations\n",
    "titles = [ \"Data Analyst\"]\n",
    "\n",
    "locations = [\"Raleigh-Durham, NC\", \"Charlotte, NC\", \"Roanoke, VA\", \"Charlottesville, VA\",\n",
    "             \"Greensboro, NC\"]\n",
    "\n",
    "query = \"\"\"I use python to collect and scrape data from the web. I can set up integrated data pipelines\n",
    "        pipeline to collect data from different sources. I train machine learning models using sklearn, \n",
    "        and tensorflow with keras. BeautifulSoup and Selenium. BeautifulSoup and Selenium.\n",
    "        BeautifulSoup and Selenium. BeautifulSoup and Selenium. I can give results to developers using Flask apps\n",
    "        and Flask APIs API. I can access APIs API and RSS feeds. I can also use SQL, particularly ElephantSQL\n",
    "        and Postgres. I like venture capital, finance and business consulting. I love to work with\n",
    "        natural language processing. Looking for a junior or entry level entry-level or mid level mid-level\n",
    "        venture capital, finance and business consulting venture capital, finance and business consulting\n",
    "        venture capital, finance and business consulting venture capital, finance and business consulting\"\"\"\n",
    "\n",
    "\n",
    "###############################################\n",
    "########### Scrape Job Listings ###############\n",
    "###############################################\n",
    "\n",
    "# Instantiate dirty_jobs\n",
    "dirty_jobs = []\n",
    "\n",
    "# Crawl over indeed.com\n",
    "print(\"Crawling Indeed:\")\n",
    "jobs = indeed_crawl(titles, locations, dirty_jobs, browser)\n",
    "\n",
    "# Clean up the text from the lxml\n",
    "print('Parsing descriptions...')\n",
    "print(\"You can close that browser now\")\n",
    "texts = parse(jobs)\n",
    "texts = [str(text)[1:-1] for text in texts]\n",
    "\n",
    "\n",
    "# Send to df\n",
    "df = pd.DataFrame(texts, columns = ['description'])\n",
    "df['jobs'] = jobs\n",
    "\n",
    "# NLP Model\n",
    "print('Loading Natural Language model...')\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "print('Done loading model!')\n",
    "\n",
    "# DO NOT MOVE THESE FUNCTIONS TO functions.py!!!\n",
    "def tokenize(text):\n",
    "    # Tokenize\n",
    "    global nlp\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]\n",
    "    if ' ' in tokens:\n",
    "        tokens.remove(' ')\n",
    "    if '  ' in tokens:\n",
    "        tokens.remove('  ')\n",
    "    return tokens\n",
    "\n",
    "print(\"Tokenizing the data...\")\n",
    "\n",
    "# send clean text to list\n",
    "text = df['description'].apply(clean_description).apply(tokenize).tolist()\n",
    "# join tokens for vectorizer\n",
    "text = [\" \".join(entry) for entry in text]\n",
    "\n",
    "# Instantiate Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "# Instantiate Vectorizer\n",
    "print('Fitting vectorizer...')\n",
    "# Create a vocab and get word counts per doc\n",
    "sparse = tfidf.fit_transform(text)\n",
    "# send to df\n",
    "tfidf_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())\n",
    "\n",
    "# Instantiate model\n",
    "print('Teaching the computer...')\n",
    "nn = NearestNeighbors(n_neighbors=20, algorithm='ball_tree')\n",
    "nn.fit(tfidf_dtm)\n",
    "print('Damn, that computer is smart.')\n",
    "\n",
    "# DO NOT REMOVE THIS FUNCTION\n",
    "def transform_query_for_nn(string):\n",
    "    # Create a vocab and get word counts per doc\n",
    "    global tfidf\n",
    "    sparse = tfidf.transform([query])\n",
    "    query_dtm = pd.DataFrame(sparse.todense(), columns = tfidf.get_feature_names())\n",
    "    return query_dtm\n",
    "\n",
    "# Process query for the model\n",
    "print('Asking the computer for recommendations...')\n",
    "query_dtm = transform_query_for_nn(query)\n",
    "\n",
    "# Query for closest neighbors\n",
    "results = nn.kneighbors(query_dtm)[1][0].tolist()\n",
    "\n",
    "# Send to list\n",
    "job_urls = df['jobs'][results].tolist()\n",
    "\n",
    "print('Done!')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_jobs(jobs, browser):\n",
    "    # # Create new Instance of Chrome in incognito mode\n",
    "    # open_browser()\n",
    "    # browser = webdriver.Firefox(executable_path='../../Selenium/geckodriver')\n",
    "\n",
    "    # # First job\n",
    "    # browser.get(jobs[0])\n",
    "\n",
    "    # Loop over the rest\n",
    "    for job in jobs:\n",
    "        browser = webdriver.Firefox(executable_path='../../Selenium/geckodriver')\n",
    "        browser.get(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_jobs(job_urls, browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
